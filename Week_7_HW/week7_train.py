# -*- coding: utf-8 -*-
"""ML APP Week 7 (IEEE-CIS-Fraud Detection)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1efcAELyUwrKc1fn0_aVlnYzwC7akTro4

# Basic Setup, Downloading data, etc.

## Imports
"""


# pip3 install kaggle --upgrade
# echo "{\"username\":\"seinberg\",\"key\":\"15a6521926e0c1ee0b073c06b3eded1e\"}" > kaggle.json
# sudo mkdir -p ~/.kaggle
# sudo cp /content/kaggle.json ~/.kaggle/kaggle.json
# chmod 600 /root/.kaggle/kaggle.json
# kaggle --version


#bash code to install libraries to import:
#pip3 install seaborn matplotlib cycler tensorflow

import numpy as np
# import seaborn as sns
import pandas as pd
# import matplotlib.pylab as plt
# from cycler import cycler
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score

# import tensorflow as tf
from sklearn.linear_model import LogisticRegression
# from tensorflow.keras.layers import Input, Dense, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import SGD,Nadam,Adam

"""## Configure kaggle"""









"""## Download data"""

# !kaggle competitions download -c ieee-fraud-detection
# !unzip ieee-fraud-detection.zip

"""### Read in data"""

np.random.seed(314159)
train_txn = pd.read_csv('Week_7_HW/train_transaction.csv')

"""# Starting David's Work

#Getting the right columns to use
"""

x_df = pd.get_dummies(train_txn.copy())
arr = pd.DataFrame(x_df.corrwith(x_df['isFraud']))
corrs = []
for label, i in arr.itertuples():
  if i > .075 or i < -.075:
    corrs.append(label)
corrs = corrs[1:]
# print(corrs)
#from dave's colab
_KEEP_COLUMNS_MODEL_ = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 
                      'card5', 'card6', 'addr1', 'addr2', 'dist1', 'P_emaildomain', 'R_emaildomain']
_TOP_COLUMNS_FROM_EDA = ["V257","V246","V244","V242","V201","V200","V189","V188","V258",       
"V45","V158","V156","V149","V228","V44","V86","V87","V170","V147","V52"]
#from finding other high correlation columns after onehotting, these are the og columns to add
_additional_columns = corrs + ['M1','M2','M3','M4']
#from finding other high correlation columns after onehotting
cols_after_dummies = ['ProductCD_C','ProductCD_W','card6_credit','card6_debit','R_emaildomain_gmail.com','M1_T','M2_T','M3_T','M4_M2',]
kept_cols = _KEEP_COLUMNS_MODEL_ + _TOP_COLUMNS_FROM_EDA + _additional_columns

for i in _KEEP_COLUMNS_MODEL_:
  if i not in corrs:
    corrs.append(i)
for i in _TOP_COLUMNS_FROM_EDA:
  if i not in corrs:
    corrs.append(i)

for i in cols_after_dummies:
  corrs.remove(i)

corrs

# #Check to see how many Nans are each in col
# for i in x_train_oh_df.columns: 
#   print(str(i) + " has " + str(x_train_oh_df[i].isna().sum()) + " number of nans")

"""from output:
* card2 has 6652 number of nans
* card3 has 1166 number of nans
* card5 has 3206 number of nans
* addr1 has 49143 number of nans
* addr2 has 49143 number of nans
* dist1 has 264211 number of nans
* dist2 has 414791 number of nans

Looks like dist2 is mainly nans so im going to get rid of the col.

going to use fillna() and use -1 for replacing nan

#transform the data
"""

#trying out other columns:
cols = corrs.copy()
for i in ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']:
  cols.remove(i)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

train_txn_copy = train_txn.copy()

y_df = train_txn_copy['isFraud']
x_df = train_txn_copy
# remove the target label from our training dataframe...
del x_df['isFraud']
#remove dist2 column bc it's mainly filled with nans
del x_df['dist2']


#After using google sheets (https://docs.google.com/spreadsheets/d/1YGZO8t0BfFJXizhumdIZzwPd1Mszso8u63SZfrVs6JA/edit?usp=sharing)
# to check which columns don't exist in test data, i got "charge card" and "R_emaildomain_windstream.net". 
#Turns out this is becuase of the train_test split, 
#so i changed the test_size to 0.35 for val/test from train, and for test from val, 
#and that gives 163 columns after one hotting the dataframes
# stratify on the target column to keep the approximate balance of positive examples since it's so imbalanced
x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(x_df, y_df, test_size=0.35, stratify=y_df,random_state=42)
x_val_df, x_test_df, y_val_df, y_test_df = train_test_split(x_test_df, y_test_df, test_size=0.35, stratify=y_test_df,random_state=42)

## SET ASIDE TEST DATA FOR INFERENCE SYSTEM
x_test_df, IS_x_test_df, y_test_df, IS_y_test_df = train_test_split(x_test_df, y_test_df, test_size=0.50, stratify=y_test_df,random_state=42)

#normalize - code from dave's colab
x_train_norm_df = x_train_df[corrs] #using the columns from the top
x_train_norm_df.TransactionAmt = (x_train_norm_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
#One hot code
x_train_oh_df = pd.get_dummies(x_train_norm_df)
# for i in cols:
#   print(i)
x_train_oh_df = x_train_oh_df[cols]

x_train_oh_df = x_train_oh_df.fillna(-1.5)


x_train_oh_df_scaled = x_train_oh_df.copy()
# apply scaler techniques
x_train_oh_df_scaled = scaler.fit_transform(x_train_oh_df_scaled)
  


# Validation data 
x_val_norm_df = x_val_df[corrs] #using the columns from the top
x_val_norm_df.TransactionAmt = (x_val_norm_df.TransactionAmt - x_val_df.TransactionAmt.mean()) / x_val_df.TransactionAmt.std()
#One hot code
x_val_oh_df = pd.get_dummies(x_val_norm_df)
x_val_oh_df = x_val_oh_df[cols]
x_val_oh_df = x_val_oh_df.fillna(-1.5)
x_val_oh_df_scaled = x_val_oh_df.copy()
# apply scaler techniques
scaler = StandardScaler()
x_val_oh_df_scaled = scaler.fit_transform(x_val_oh_df_scaled)

#TEST DATA
x_test_norm_df = x_test_df[corrs]
x_test_norm_df.TransactionAmt = (x_test_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
x_test_norm_df

x_test_oh_df = pd.get_dummies(x_test_norm_df)
x_test_oh_df = x_test_oh_df[cols]
x_test_oh_df = x_test_oh_df.fillna(-1.5)

x_test_oh_df_scaled = x_test_oh_df.copy()
scaler = StandardScaler()
x_test_oh_df_scaled = scaler.fit_transform(x_test_oh_df_scaled)


# print(len(x_train_oh_df_scaled[0]))
# print(len(x_val_oh_df_scaled[0]))
# print(len(x_test_oh_df_scaled[0]))

# from google.colab import files
# IS_x_test_df.to_csv('IS_x_test_df.csv', encoding='utf-8-sig') 
# files.download('IS_x_test_df.csv')
# IS_y_test_df.to_csv('IS_y_test_df.csv', encoding='utf-8-sig') 
# files.download('IS_y_test_df.csv')
# len(IS_x_test_df)

# from sklearn.preprocessing import StandardScaler
# scaler = StandardScaler()

# train_txn_copy = train_txn.copy()

# y_df = train_txn_copy['isFraud']
# x_df = train_txn_copy
# # remove the target label from our training dataframe...
# del x_df['isFraud']
# #remove dist2 column bc it's mainly filled with nans
# del x_df['dist2']


# #After using google sheets (https://docs.google.com/spreadsheets/d/1YGZO8t0BfFJXizhumdIZzwPd1Mszso8u63SZfrVs6JA/edit?usp=sharing)
# # to check which columns don't exist in test data, i got "charge card" and "R_emaildomain_windstream.net". 
# #Turns out this is becuase of the train_test split, 
# #so i changed the test_size to 0.35 for val/test from train, and for test from val, 
# #and that gives 163 columns after one hotting the dataframes
# # stratify on the target column to keep the approximate balance of positive examples since it's so imbalanced
# x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(x_df, y_df, test_size=0.35, stratify=y_df,random_state=42)
# x_val_df, x_test_df, y_val_df, y_test_df = train_test_split(x_test_df, y_test_df, test_size=0.35, stratify=y_test_df,random_state=42)
# #normalize - code from dave's colab
# x_train_norm_df = x_train_df[kept_cols] #using the columns from the top
# x_train_norm_df.TransactionAmt = (x_train_norm_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
# #One hot code
# x_train_oh_df = pd.get_dummies(x_train_norm_df)
# x_train_oh_df = x_train_oh_df.fillna(-1.5)
# x_train_oh_df_scaled = x_train_oh_df.copy()


# # apply scaler techniques
# x_train_oh_df_scaled = scaler.fit_transform(x_train_oh_df_scaled)
  


# # Validation data 
# x_val_norm_df = x_val_df[kept_cols] #using the columns from the top
# x_val_norm_df.TransactionAmt = (x_val_norm_df.TransactionAmt - x_val_df.TransactionAmt.mean()) / x_val_df.TransactionAmt.std()
# #One hot code
# x_val_oh_df = pd.get_dummies(x_val_norm_df)
# x_val_oh_df = x_val_oh_df.fillna(-1.5)
# x_val_oh_df_scaled = x_val_oh_df.copy()
# # apply scaler techniques
# scaler = StandardScaler()
# x_val_oh_df_scaled = scaler.fit_transform(x_val_oh_df_scaled)

# #TEST DATA
# x_test_norm_df = x_test_df[kept_cols]
# x_test_norm_df.TransactionAmt = (x_test_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
# x_test_norm_df

# x_test_oh_df = pd.get_dummies(x_test_norm_df)
# x_test_oh_df = x_test_oh_df.fillna(-1.5)

# x_test_oh_df_scaled = x_test_oh_df.copy()
# scaler = StandardScaler()
# x_test_oh_df_scaled = scaler.fit_transform(x_test_oh_df_scaled)


# # print(len(x_train_oh_df_scaled[0]))
# # print(len(x_val_oh_df_scaled[0]))
# # print(len(x_test_oh_df_scaled[0]))

for i in x_train_oh_df.columns:
  print(i)

print(set(x_test_oh_df.columns) - set(x_train_oh_df.columns))
print(x_test_oh_df.shape)
print(x_train_oh_df.shape)

print(len(x_train_oh_df_scaled[0]))
print(len(x_val_oh_df_scaled[0]))
print(len(x_test_oh_df_scaled[0]))

"""#### xgb"""

# import xgboost as xgb
# from sklearn.metrics import precision_score
# from sklearn.metrics import recall_score
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import roc_auc_score

"""Training the model

##note, the colab ran out of GPU usage, so it's not saved as output but the best the XGBoost could do was just under 70% ROC AUC
"""

# import xgboost as xgb
# from xgboost import plot_importance
# import matplotlib.pyplot as plt
# %matplotlib inline

# roc_scores = []
# accuracy_scores = []
# precision_scores = []
# recall_scores = []



# n_estimators = [5000]
# max_depths = [20]
# learning_rates = [0.01]


# for n_estimator in n_estimators:
#   for max_depth in max_depths:
#     for learning_rate in learning_rates:
#       print("with n_estimators: " + str(n_estimator))
#       print("with max_depth: " + str(max_depth))
#       print("with learning_rate: " + str(learning_rate))
#       print("")
#       clf = xgb.XGBClassifier(
#           n_estimators=n_estimator,
#           max_depth=max_depth,
#           learning_rate=learning_rate,
#           subsample=0.9,
#           colsample_bytree=0.9,
#           missing=-1.5,
#           random_state=2,
#           tree_method='gpu_hist' # set runtime to gpu to take advantage
#       )
#       xgb_clf = clf.fit(x_train_oh_df_scaled, y_train_df)


#       # plt.rcParams['figure.figsize'] = [30, 15]
#       # plot_importance(clf, max_num_features=30)
#       # plt.show()

#       xgb_predict = xgb_clf.predict(x_test_oh_df_scaled)
      
#       acc_score = accuracy_score(y_test_df, xgb_predict)
#       print("accuracy score: "  + str(acc_score))
#       accuracy_scores.append(acc_score)

#       prec_score = precision_score(y_test_df, xgb_predict)
#       print("precision score: "  + str())
#       precision_scores.append(prec_score)

#       rec_scores = recall_score(y_test_df, xgb_predict)
#       print("recall score: "  + str(rec_scores))
#       recall_scores.append(rec_scores)

#       print("confusion matrix: "  + str(confusion_matrix(y_test_df, xgb_predict)))

#       roc_score = roc_auc_score(y_test_df, xgb_predict)
#       roc_scores.append(roc_score)
#       print("ROC AUC Score: "  + str(roc_score))
#       print("")

# max(roc_scores)

# from xgboost import plot_importance
# import matplotlib.pyplot as plt
# %matplotlib inline

# plt.rcParams['figure.figsize'] = [30, 15]
# plot_importance(clf, max_num_features=30)
# plt.show()

"""#### Running with Neural Nets"""

input_shape = x_train_oh_df.values.shape
input_shape

input_shape = Input(shape=(x_train_oh_df.values.shape[1],))
model = tf.keras.models.Sequential([
     tf.keras.layers.Dense(128, activation = 'tanh',input_shape=(140,)),
     tf.keras.layers.Dense(64, activation = 'tanh'),
     tf.keras.layers.Dense(32, activation = 'tanh'),
     tf.keras.layers.Dense(16, activation = 'tanh'),
     tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])

# inputs = Input(shape=(x_train_oh_df.values.shape[1],))
# x = tf.keras.layers.Dense(4, activation=tf.nn.tanh)(inputs)
# preds = Dense(1, activation='sigmoid')(x)
# model = Model(inputs=inputs, outputs=preds)
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
history = model.fit(x_train_oh_df_scaled, y_train_df.values, validation_data=[x_val_oh_df_scaled,y_val_df], batch_size=256, epochs=25, shuffle=False)

print(history.history.keys()) # graph maker taken from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/ 
# graph accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('accuracy')
plt.ylabel('val_accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()
# graph loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='best')
plt.show()

y_pred_k = model.predict(x_test_oh_df_scaled)

type(y_pred_k)

temp = []
for i in np.round(y_pred_k.copy()):
  temp.append(int(i))

from sklearn.metrics import classification_report
rounded = temp
print(classification_report(y_test_df, temp))

roc_auc_score(y_test_df, y_pred_k)

"""Graph generation from https://stackoverflow.com/questions/65249043/difference-between-sklearn-roc-auc-score-and-sklearn-plot-roc-curve """

from sklearn.metrics import roc_curve, auc
fpr, tpr, _ = roc_curve(y_test_df, y_pred_k)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label='AUC = ' + str(round(roc_auc, 2)))
plt.legend(loc='lower right')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

"""# ROC SCORE OF 0.88 !!!

#Saving the model
"""

model.save('week_7_model_ROC_88.h5')
from google.colab import files
files.download('week_7_model_ROC_88.h5')

"""#now trying SimpleImputer instead of fillna()"""

from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')

from imblearn.over_sampling import SMOTE
#my thought is that random state should be the same as for train test split
sm = SMOTE(random_state = 42)

#trying out other columns:
cols = corrs.copy()
for i in ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']:
  cols.remove(i)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

train_txn_copy = train_txn.copy()

y_df = train_txn_copy['isFraud']
x_df = train_txn_copy
# remove the target label from our training dataframe...
del x_df['isFraud']
#remove dist2 column bc it's mainly filled with nans
del x_df['dist2']


#After using google sheets (https://docs.google.com/spreadsheets/d/1YGZO8t0BfFJXizhumdIZzwPd1Mszso8u63SZfrVs6JA/edit?usp=sharing)
# to check which columns don't exist in test data, i got "charge card" and "R_emaildomain_windstream.net". 
#Turns out this is becuase of the train_test split, 
#so i changed the test_size to 0.35 for val/test from train, and for test from val, 
#and that gives 163 columns after one hotting the dataframes
# stratify on the target column to keep the approximate balance of positive examples since it's so imbalanced
x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(x_df, y_df, test_size=0.35, stratify=y_df,random_state=42)
x_val_df, x_test_df, y_val_df, y_test_df = train_test_split(x_test_df, y_test_df, test_size=0.35, stratify=y_test_df,random_state=42)
#normalize - code from dave's colab
x_train_norm_df = x_train_df[corrs] #using the columns from the top
x_train_norm_df.TransactionAmt = (x_train_norm_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
#One hot code
x_train_oh_df = pd.get_dummies(x_train_norm_df)
# for i in cols:
#   print(i)
x_train_oh_df = x_train_oh_df[cols]
#simpleImputer
x_train_oh_df = imp_mean.fit_transform(x_train_oh_df)
# x_train_oh_df = x_train_oh_df.fillna(-1.5)
x_train_oh_df_scaled = x_train_oh_df.copy()
# apply scaler techniques
x_train_oh_df_scaled = scaler.fit_transform(x_train_oh_df_scaled)

#SMOTE
# x_train_oh_df, y_train_df = sm.fit_resample(x_train_oh_df,y_train_df)


  


# Validation data 
x_val_norm_df = x_val_df[corrs] #using the columns from the top
x_val_norm_df.TransactionAmt = (x_val_norm_df.TransactionAmt - x_val_df.TransactionAmt.mean()) / x_val_df.TransactionAmt.std()
#One hot code
x_val_oh_df = pd.get_dummies(x_val_norm_df)
x_val_oh_df = x_val_oh_df[cols]
# x_val_oh_df = x_val_oh_df.fillna(-1.5)
#simpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
x_val_oh_df = imp_mean.fit_transform(x_val_oh_df)

# apply scaler techniques
x_val_oh_df_scaled = x_val_oh_df.copy()
scaler = StandardScaler()
x_val_oh_df_scaled = scaler.fit_transform(x_val_oh_df_scaled)
#SMOTE
# x_val_oh_df, y_val_df = sm.fit_resample(x_val_oh_df,y_val_df)

#TEST DATA
x_test_norm_df = x_test_df[corrs]
x_test_norm_df.TransactionAmt = (x_test_df.TransactionAmt - x_train_df.TransactionAmt.mean()) / x_train_df.TransactionAmt.std()
x_test_norm_df

x_test_oh_df = pd.get_dummies(x_test_norm_df)
x_test_oh_df = x_test_oh_df[cols]
# x_test_oh_df = x_test_oh_df.fillna(-1.5)

#simpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
x_test_oh_df = imp_mean.fit_transform(x_test_oh_df)

# apply scaler techniques
x_test_oh_df_scaled = x_test_oh_df.copy()
scaler = StandardScaler()
x_test_oh_df_scaled = scaler.fit_transform(x_test_oh_df_scaled)

#SMOTE
# x_test_oh_df, y_test_df = sm.fit_resample(x_test_oh_df,y_test_df)


print(len(x_train_oh_df_scaled[0]))
print(len(x_val_oh_df_scaled[0]))
print(len(x_test_oh_df_scaled[0]))

#simpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')
x_test_oh_df = imp_mean.fit_transform(x_test_oh_df)

# apply scaler techniques
x_test_oh_df_scaled = x_test_oh_df.copy()
scaler = StandardScaler()
x_test_oh_df_scaled = scaler.fit_transform(x_test_oh_df_scaled)

x_test_oh_df_scaled



# input_shape = Input(shape=(x_train_oh_df.values.shape[1],))
model = tf.keras.models.Sequential([
     tf.keras.layers.Dense(512, activation = 'tanh',input_shape=(140,)),
     tf.keras.layers.Dense(256, activation = 'tanh'),
     tf.keras.layers.Dense(128, activation = 'tanh'),
     tf.keras.layers.Dense(64, activation = 'tanh'),
     tf.keras.layers.Dense(32, activation = 'tanh'),
     tf.keras.layers.Dense(16, activation = 'tanh'),
     tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])

# inputs = Input(shape=(x_train_oh_df.values.shape[1],))
# x = tf.keras.layers.Dense(4, activation=tf.nn.tanh)(inputs)
# preds = Dense(1, activation='sigmoid')(x)
# model = Model(inputs=inputs, outputs=preds)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
model.summary()
def scheduler(epoch, lr):
   if epoch < 5:
     return lr
   elif epoch > 13:
      return lr
   else:
     return lr * tf.math.exp(-0.1)
callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

history = model.fit(x_train_oh_df_scaled, y_train_df.values, validation_data=[x_val_oh_df_scaled,y_val_df],callbacks =[callback], batch_size=256, epochs=25, shuffle=False)



print(history.history.keys()) # graph maker taken from https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/ 
# graph accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('accuracy')
plt.ylabel('val_accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()
# graph loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='best')
plt.show()

y_pred_k = model.predict(x_test_oh_df_scaled)

y_pred_k

temp = []
for i in np.round(y_pred_k.copy()):
  temp.append(int(i))

from sklearn.metrics import classification_report
rounded = temp
print(classification_report(y_test_df, temp))

roc_auc_score(y_test_df, y_pred_k)

"""Graph generation from https://stackoverflow.com/questions/65249043/difference-between-sklearn-roc-auc-score-and-sklearn-plot-roc-curve """

from sklearn.metrics import roc_curve, auc
fpr, tpr, _ = roc_curve(y_test_df, y_pred_k)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label='AUC = ' + str(round(roc_auc, 2)))
plt.legend(loc='lower right')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')



y_pred_k = y_pred_k > 0.5
accuracy_score(y_test_df, np.round(y_pred_k))